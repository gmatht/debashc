use logos::Logos;
use thiserror::Error;
use std::cmp::Ordering;

#[derive(Logos, Debug, PartialEq, Clone)]
pub enum Token {
    // Keywords
    #[token("if")]
    If,
    #[token("then")]
    Then,
    #[token("else")]
    Else,
    #[token("elif")]
    Elif,
    #[token("fi")]
    Fi,
    #[token("while")]
    While,
    #[token("do")]
    Do,
    #[token("done")]
    Done,
    #[token("for")]
    For,
    #[token("in")]
    In,
    #[token("function")]
    Function,
    #[token("case")]
    Case,
    #[token("esac")]
    Esac,
    #[token("select")]
    Select,
    #[token("until")]
    Until,
    #[token("break")]
    Break,
    #[token("continue")]
    Continue,
    #[token("return")]
    Return,
    #[token("exit")]
    Exit,
    #[token("export")]
    Export,
    #[token("readonly")]
    Readonly,
    #[token("local")]
    Local,
    #[token("declare")]
    Declare,
    #[token("typeset")]
    Typeset,
    #[token("unset")]
    Unset,
    #[token("shift")]
    Shift,
    #[token("set")]
    Set,
    #[token("eval")]
    Eval,
    #[token("exec")]
    Exec,
    #[token("source")]
    Source,
    // SourceDot removed - dots in filenames should be part of identifiers
    #[token("trap")]
    Trap,
    #[token("wait")]
    Wait,
    #[token("shopt")]
    Shopt,
    #[token("true")]
    True,
    #[token("false")]
    False,
    #[token("[")]
    TestBracket,
    #[token("]")]
    TestBracketClose,

    // Operators
    #[token("|")]
    Pipe,
    #[token("||", priority = 1)]
    Or,
    #[token("&")]
    Background,
    #[token("&&", priority = 1)]
    And,
    #[token(";")]
    Semicolon,
    #[token(",")]
    Comma,
    #[token(";;", priority = 1)]
    DoubleSemicolon,
    #[token("..", priority = 3)]
    Range,
    #[token("(")]
    ParenOpen,
    #[token(")")]
    ParenClose,
    #[token("{")]
    BraceOpen,
    #[token("}")]
    BraceClose,
    #[token("=")]
    Assign,
    #[token("%=", priority = 3)]
    PercentAssign,
    #[token("**=", priority = 3)]
    StarStarAssign,
    #[token("<<=", priority = 3)]
    LeftShiftAssign,
    #[token(">>=", priority = 2)]
    RightShiftAssign,
    #[token("&=", priority = 3)]
    AndAssign,
    #[token("^=", priority = 3)]
    CaretAssign,
    #[token("|=", priority = 3)]
    OrAssign,

    // Redirections
    #[token("<")]
    RedirectIn,
    #[token(">>", priority = 0)]
    RedirectAppend,
    #[token(">")]
    RedirectOut,
    #[token("<>", priority = 1)]
    RedirectInOut,
    #[token("<<", priority = 1)]
    Heredoc,
    #[token("<<-", priority = 1)]
    HeredocTabs,
    #[token("<<<", priority = 1)]
    HereString,
    #[token(">&", priority = 1)]
    RedirectOutErr,
    #[token("<&", priority = 1)]
    RedirectInErr,
    #[token(">|", priority = 1)]
    RedirectOutClobber,
    #[token("&>", priority = 1)]
    RedirectAll,
    #[token("&>>", priority = 1)]
    RedirectAllAppend,

    // Variables and expansions
    #[token("$", priority = 2)]
    Dollar,
    #[token("${")]
    DollarBrace,
    #[token("$(")]
    DollarParen,
    #[token("$#", priority = 3)]
    DollarHashSimple,
    #[token("$@", priority = 3)]
    DollarAtSimple,
    #[token("$*", priority = 3)]
    DollarStarSimple,
    // Backtick token not currently used
    #[token("`", priority = 1)]
    _Backtick, // Unused variant, prefixed with underscore
    #[token("${#", priority = 3)]
    DollarBraceHash,
    #[token("${!", priority = 3)]
    DollarBraceBang,
    #[token("${*", priority = 3)]
    DollarBraceStar,
    #[token("${@", priority = 3)]
    DollarBraceAt,
    #[token("${#*", priority = 3)]
    DollarBraceHashStar,
    #[token("${#@", priority = 3)]
    DollarBraceHashAt,
    #[token("${!*", priority = 3)]
    DollarBraceBangStar,
    #[token("${!@", priority = 3)]
    DollarBraceBangAt,

    // Arithmetic
    #[token("$((", priority = 0)]
    Arithmetic,
    #[token("((", priority = 0)]
    ArithmeticEval,
    #[token("))", priority = 0)]
    ArithmeticEvalClose,
    #[token("$[")]
    ArithmeticBracket,
    #[token("let")]
    Let,

    // Conditionals
    #[token("-eq", priority = 1)]
    Eq,
    #[token("-ne", priority = 1)]
    Ne,
    #[token("-lt", priority = 1)]
    Lt,
    #[token("-le", priority = 1)]
    Le,
    #[token("-gt", priority = 1)]
    Gt,
    #[token("-ge", priority = 1)]
    Ge,
    #[token("-z", priority = 1)]
    Zero,
    #[token("-n", priority = 1)]
    NonZero,
    #[token("-f", priority = 1)]
    File,
    #[token("-d", priority = 1)]
    Directory,
    #[token("-e", priority = 1)]
    Exists,
    #[token("-r", priority = 10)]
    Readable,
    #[token("-w", priority = 1)]
    Writable,
    #[token("-x", priority = 1)]
    Executable,
    #[token("-s", priority = 1)]
    Size,
    #[token("-L", priority = 1)]
    Symlink,
    #[token("-h", priority = 1)]
    SymlinkH,
    #[token("-p", priority = 1)]
    PipeFile,
    #[token("-S", priority = 1)]
    Socket,
    #[token("-b", priority = 1)]
    Block,
    #[token("-c", priority = 1)]
    Character,
    #[token("-g", priority = 1)]
    SetGid,
    #[token("-k", priority = 1)]
    Sticky,
    #[token("-u", priority = 1)]
    SetUid,
    #[token("-O", priority = 1)]
    Owned,
    #[token("-G", priority = 1)]
    GroupOwned,
    #[token("-N", priority = 1)]
    Modified,
    #[token("-nt", priority = 1)]
    NewerThan,
    #[token("-ot", priority = 1)]
    OlderThan,
    #[token("-ef", priority = 1)]
    SameFile,

    // Command-line flags (general)
    #[token("-name")]
    NameFlag,
    #[token("-maxdepth")]
    MaxDepthFlag,
    #[token("-type")]
    TypeFlag,

    // Regex matching
    #[token("=~")]
    RegexMatch,

    // Strings and literals
    #[regex(r#""([^"\\]|\\.)*""#, priority = 4)]
    DoubleQuotedString,
    #[regex(r"'([^'\\]|\\[^'])*'", priority = 3)]
    SingleQuotedString,
    #[regex(r"`([^`\\]|\\.)*`", priority = 3)]
    BacktickString,
    #[regex(r"\$'([^'\\]|\\.)*'", priority = 3)]
    DollarSingleQuotedString,
    #[regex(r#"\$"([^"\\]|\\.)*""#, priority = 3)]
    DollarDoubleQuotedString,

    // Long options (must come before Identifier to avoid conflicts)
    #[regex(r"--[a-zA-Z][a-zA-Z0-9_*?.-]*=[^ \t\n\r|&;(){}]*", priority = 3)]
    LongOption,
    

    
    // Identifiers and words
    #[regex(r"[a-zA-Z_][a-zA-Z0-9_*?\-]*", priority = 2)]
    Identifier,

    #[regex(r"[0-9]+")]
    Number,
    #[regex(r"[0-9]+\.[0-9]+")]
    Float,
    #[regex(r"0x[0-9a-fA-F]+")]
    HexNumber,
    #[regex(r"0[0-7]+")]
    OctalNumber,

    // Special characters
    #[token("!")]
    Bang,
    // #[token("#", priority = 1)]
    // _Hash, // Unused variant, prefixed with underscore
    #[token("%", priority = 2)]
    Percent,
    #[token("^", priority = 2)]
    Caret,
    #[token("~")]
    Tilde,
    #[token("+")]
    Plus,
    #[token("+=", priority = 3)]
    PlusAssign,
    #[token("-")]
    Minus,
    #[token("-=", priority = 3)]
    MinusAssign,
    #[token("*")]
    Star,
    #[token("*=", priority = 3)]
    StarAssign,
    #[token("/")]
    Slash,
    #[token("/=", priority = 3)]
    SlashAssign,
    #[token("\\", priority = 1)]
    _Backslash, // Unused variant, prefixed with underscore
    #[token("?")]
    Question,
    #[token(".")]
    Dot,
    #[regex(r"\*[a-zA-Z0-9_*?.-]*|\[[a-zA-Z0-9\-]+\]|\[[a-zA-Z0-9\-]+\]\[[a-zA-Z0-9\-]+\]", priority = 1)]
    CasePattern,
    #[token(":", priority = 1)]
    Colon,
    #[token("@")]
    At,
    #[token("`", priority = 2)]
    BacktickChar,
    #[token("'")]
    SingleQuote,
    #[token("\"")]
    DoubleQuote,
    #[token("\\", priority = 2)]
    Escape,
    #[regex(r"\n", priority = 3)]
    Newline,
    #[token("\r")]
    CarriageReturn,
    #[token("\t")]
    Tab,
    #[regex(r" +", priority = 3)]
    Space,

    // Comments
    #[regex(r"#[^\r\n]*", priority = 10)]
    Comment,
    
    // Regex pattern content (for bash test expressions)
    #[regex(r"\^[0-9\-\[\]\+\.\$\*\(\)\?\\|]+", priority = 1)]
    RegexPattern,
}

#[derive(Error, Debug)]
pub enum LexerError {
    #[error("Unexpected character: {ch} at {line}:{col}")]
    UnexpectedChar { ch: char, line: usize, col: usize },
    #[error("Unterminated string")]
    _UnterminatedString, // Unused variant, prefixed with underscore
    #[error("Invalid escape sequence")]
    _InvalidEscape, // Unused variant, prefixed with underscore
}

pub struct Lexer {
    pub tokens: Vec<(Token, usize, usize)>,
    pub current: usize,
    pub input: String,
    pub line_starts: Vec<usize>,
}

impl Lexer {
    pub fn new(input: &str) -> Self {
        let mut tokens = Vec::new();
        let mut lexer = Token::lexer(input);
        
        while let Some(token_result) = lexer.next() {
            let span = lexer.span();
            match token_result {
                Ok(token) => tokens.push((token, span.start, span.end)),
                Err(_) => {
                    // Skip invalid tokens
                    continue;
                }
            }
        }
        
        // Precompute starts of lines for quick offset->(line,col)
        let mut line_starts = Vec::new();
        line_starts.push(0);
        let mut i = 0;
        while i < input.len() {
            if input.as_bytes()[i] == b'\r' && i + 1 < input.len() && input.as_bytes()[i + 1] == b'\n' {
                // Windows line ending: \r\n - only count \n as line break
                if i + 2 < input.len() {
                    line_starts.push(i + 2);
                }
                i += 2;
            } else if input.as_bytes()[i] == b'\n' {
                // Unix line ending: \n
                if i + 1 < input.len() {
                    line_starts.push(i + 1);
                }
                i += 1;
            } else if input.as_bytes()[i] == b'\r' {
                // Lone \r (old Mac line ending)
                if i + 1 < input.len() {
                    line_starts.push(i + 1);
                }
                i += 1;
            } else {
                i += 1;
            }
        }

        Self { tokens, current: 0, input: input.to_string(), line_starts }
    }

    pub fn peek(&self) -> Option<&Token> {
        self.tokens.get(self.current).map(|(token, _, _)| token)
    }

pub fn next(&mut self) -> Option<&Token> {
        let token = self.tokens.get(self.current).map(|(token, _, _)| token);
        self.current += 1;
        token
    }

    pub fn consume(&mut self, expected: Token) -> Result<(), LexerError> {
        if let Some(token) = self.next() {
            if std::mem::discriminant(token) == std::mem::discriminant(&expected) {
                Ok(())
            } else {
                // Get the actual character from the current token for better error reporting
                // Note: self.current was incremented by next(), so we need to look at current - 1
                if let Some((_, start, end)) = self.tokens.get(self.current - 1) {
                    let actual_char = self.input[*start..*end].chars().next().unwrap_or('?');
                    let (line, col) = self.offset_to_line_col(*start);
                    Err(LexerError::UnexpectedChar { ch: actual_char, line, col })
                } else {
                    Err(LexerError::UnexpectedChar { ch: '?', line: 1, col: 1 })
                }
            }
        } else {
            Err(LexerError::UnexpectedChar { ch: '?', line: 1, col: 1 })
        }
    }

pub fn get_current_text(&self) -> Option<String> {
        self.tokens.get(self.current).map(|(_, start, end)| {
            self.input[*start..*end].to_string()
        })
    }
    

}

impl Lexer {

    pub fn offset_to_line_col(&self, offset: usize) -> (usize, usize) {
        if self.line_starts.is_empty() {
            return (1, offset + 1);
        }
        // Binary search for the greatest line_start <= offset
        let mut left = 0usize;
        let mut right = self.line_starts.len();
        while left < right {
            let mid = (left + right) / 2;
            match self.line_starts[mid].cmp(&offset) {
                Ordering::Greater => right = mid,
                _ => left = mid + 1,
            }
        }
        let idx = left.saturating_sub(1);
        let line_start = self.line_starts.get(idx).cloned().unwrap_or(0);
        let line = idx + 1;
        let col = offset.saturating_sub(line_start) + 1;
        (line, col)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_basic_tokens() {
        let input = "echo hello world";
        let mut lexer = Lexer::new(input);
        
        assert_eq!(lexer.next(), Some(&Token::Identifier));
        assert_eq!(lexer.next(), Some(&Token::Space));
        assert_eq!(lexer.next(), Some(&Token::Identifier));
        assert_eq!(lexer.next(), Some(&Token::Space));
        assert_eq!(lexer.next(), Some(&Token::Identifier));
        assert_eq!(lexer.next(), None);
    }

    #[test]
    fn test_pipeline() {
        let input = "ls | grep test";
        let mut lexer = Lexer::new(input);
        
        assert_eq!(lexer.next(), Some(&Token::Identifier));
        assert_eq!(lexer.next(), Some(&Token::Space));
        assert_eq!(lexer.next(), Some(&Token::Pipe));
        assert_eq!(lexer.next(), Some(&Token::Space));
        assert_eq!(lexer.next(), Some(&Token::Identifier));
        assert_eq!(lexer.next(), Some(&Token::Space));
        assert_eq!(lexer.next(), Some(&Token::Identifier));
    }

    #[test]
    fn test_variables() {
        let input = "$HOME ${PATH}";
        let mut lexer = Lexer::new(input);
        
        assert_eq!(lexer.next(), Some(&Token::Dollar));
        assert_eq!(lexer.next(), Some(&Token::Identifier));
        assert_eq!(lexer.next(), Some(&Token::Space));
        assert_eq!(lexer.next(), Some(&Token::DollarBrace));
        assert_eq!(lexer.next(), Some(&Token::Identifier));
        assert_eq!(lexer.next(), Some(&Token::BraceClose));
    }
} 

